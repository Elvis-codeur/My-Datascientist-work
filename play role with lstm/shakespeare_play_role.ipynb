{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOCiK7j-LCvZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.datasets import imdb\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "import os \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "metadata": {
        "id": "pc6CUysELE5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fk2dTxHSLUkw",
        "outputId": "5399622a-84c5-4e9c-fe52-829d076ac6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/shakespeare.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "vpqYhIrFLWTX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "731e1151-c8cb-4721-b004-08a54c24784f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4a0f2618-e00b-4012-a306-6ece4f0313cd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4a0f2618-e00b-4012-a306-6ece4f0313cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f0cf22c78c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpath_to_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m   \"\"\"\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    144\u001b[0m   result = _output.eval_js(\n\u001b[1;32m    145\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m--> 146\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14C5e9oKLziG",
        "outputId": "6850897c-2638-4e03-9a9b-a66d4c3201c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N42E0tdmL6vo",
        "outputId": "0ec7744f-96a8-462f-e9de-19ae1415a744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "wKJGYdRVL99N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpMKmZHgMEJh",
        "outputId": "e0b70572-b08a-4e84-f0f1-97d07ebfbb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQrPn4TXMHT1",
        "outputId": "422a15a6-ef49-43c4-c20c-b6de5625348f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "p40WQQNQMK4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "hoQMoYUXMOeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "pKiPUSkoMQ4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf_5ZZQ9MUfg",
        "outputId": "e0d06cd7-c73f-4be5-c788-69a43a63938e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "XEt8VKQVMXMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8crWn1QmMcvK",
        "outputId": "21ff7215-3d97-4b26-ec5b-6a48ef3b8a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "id": "X0OLhMZjMk-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d82096-f554-4681-b486-74860ca7a2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "id": "ogO7ByI933wr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ad9dff-4b63-4dfc-8807-ae5692c338f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 6.5990896e-03 -8.0491661e-04  2.9588069e-03 ...  1.7123790e-03\n",
            "    4.9040182e-03  8.1748265e-04]\n",
            "  [ 2.9765735e-03  2.8438934e-03 -1.7314260e-03 ...  1.8159207e-03\n",
            "    5.7474291e-04  1.5131948e-03]\n",
            "  [ 8.3435391e-04 -8.3201716e-04  2.9813726e-03 ... -1.1281332e-03\n",
            "    1.9498751e-03 -1.3327150e-04]\n",
            "  ...\n",
            "  [ 6.7952625e-03 -1.5051133e-03  5.0865868e-03 ...  5.4934071e-03\n",
            "   -8.4502632e-03 -3.2617158e-04]\n",
            "  [ 6.1084516e-05  6.2189263e-04  7.4191783e-03 ...  5.2488940e-03\n",
            "   -4.2562382e-03 -3.0917411e-03]\n",
            "  [-1.5618687e-03  2.3696423e-03  2.1944353e-03 ...  5.2982434e-03\n",
            "   -7.8450162e-03 -2.3223278e-03]]\n",
            "\n",
            " [[-4.1093596e-04 -3.6234241e-03 -1.9878540e-03 ...  2.6713074e-03\n",
            "    2.4149560e-03 -1.1510878e-03]\n",
            "  [-2.1525826e-03 -6.1838338e-03  3.4255113e-03 ... -1.7863679e-03\n",
            "    3.2194802e-03 -3.1873665e-03]\n",
            "  [-2.4450489e-03 -1.5695164e-03 -1.2125322e-03 ...  3.1254345e-04\n",
            "   -1.7675369e-03 -9.5481437e-04]\n",
            "  ...\n",
            "  [ 6.0572159e-03 -1.2118630e-02  1.0903649e-02 ...  6.4004436e-03\n",
            "   -2.2750443e-03 -4.3609063e-03]\n",
            "  [ 7.4998261e-03 -1.3084317e-02  8.8380994e-03 ...  1.1551888e-03\n",
            "   -3.0416711e-03 -4.3715211e-04]\n",
            "  [ 4.5614447e-03 -6.5098517e-03  4.7636451e-03 ...  2.9557396e-03\n",
            "   -6.3560810e-03  5.5813615e-04]]\n",
            "\n",
            " [[-7.2830259e-03 -5.9107440e-03 -2.4016011e-03 ...  7.0054741e-03\n",
            "   -3.8141379e-04  4.9557164e-04]\n",
            "  [-8.1295976e-03 -3.5012234e-03 -8.9810567e-04 ...  1.7534192e-03\n",
            "   -6.9595454e-03  5.6988467e-03]\n",
            "  [-8.4514627e-03 -2.0561321e-03  9.9288067e-05 ... -2.0947382e-03\n",
            "   -1.1907969e-02  8.8837920e-03]\n",
            "  ...\n",
            "  [ 6.6153789e-03 -2.3345451e-03  4.1162143e-03 ...  5.0649978e-05\n",
            "    3.3951120e-03 -6.1986642e-03]\n",
            "  [ 3.7491634e-03 -4.1383663e-03  6.3685556e-03 ...  2.6816712e-03\n",
            "   -2.1977131e-03 -7.4889110e-03]\n",
            "  [ 1.5645183e-03 -6.1662809e-04  2.0564550e-03 ...  2.1488005e-03\n",
            "   -5.9603760e-03 -3.1377545e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 4.6238210e-03  5.7089105e-03  2.6397035e-04 ...  3.9223959e-03\n",
            "   -1.4505622e-03 -7.9187984e-04]\n",
            "  [ 3.1503413e-03  6.1946982e-03  6.9119404e-03 ... -3.2227901e-03\n",
            "    3.0886394e-04  9.3814498e-04]\n",
            "  [ 1.2220021e-03 -2.1626067e-04  2.4342469e-03 ... -4.6885578e-04\n",
            "    1.6098617e-03 -3.2637807e-04]\n",
            "  ...\n",
            "  [ 4.7664775e-04  1.9576799e-03 -1.2881856e-04 ...  3.9708326e-03\n",
            "   -1.2245777e-03 -1.2392416e-03]\n",
            "  [ 6.0916971e-04 -1.8426408e-03 -1.0145714e-03 ...  6.6964133e-03\n",
            "   -4.4317893e-04 -2.5282311e-03]\n",
            "  [-1.3570616e-03  1.1215890e-03  1.3769495e-03 ...  4.1429717e-03\n",
            "   -5.3158128e-03 -4.7237226e-03]]\n",
            "\n",
            " [[-1.9465650e-03  3.4127168e-03 -4.1044983e-03 ...  1.0999818e-03\n",
            "   -2.9538083e-03  8.5257553e-04]\n",
            "  [ 1.5848719e-03 -1.5210898e-03 -5.5328482e-03 ... -2.4977266e-03\n",
            "   -3.3488325e-03  2.7197760e-03]\n",
            "  [ 8.7867593e-03 -2.1025517e-03 -1.2778338e-03 ...  1.0431311e-03\n",
            "    1.6197211e-03  2.2750760e-03]\n",
            "  ...\n",
            "  [ 7.9064164e-03 -8.0858357e-04  8.8240616e-03 ...  3.2807332e-03\n",
            "   -6.3418504e-04  2.4783977e-03]\n",
            "  [ 1.2555845e-02 -3.2374724e-03  7.9409070e-03 ...  6.2149856e-03\n",
            "   -4.0899222e-03 -3.2866255e-03]\n",
            "  [ 1.4623936e-02  2.2686776e-03  7.8181820e-03 ...  8.6233774e-03\n",
            "   -6.2780278e-03 -3.2574125e-03]]\n",
            "\n",
            " [[-2.9248265e-03 -7.4027996e-03  5.2197473e-03 ... -9.8709879e-04\n",
            "    2.2654268e-03 -7.1698212e-04]\n",
            "  [-2.0343254e-03 -1.3704953e-03  3.6288081e-03 ... -5.1045525e-03\n",
            "    3.4941547e-04 -4.1796183e-03]\n",
            "  [-2.4681876e-03 -4.8091901e-03  1.5170507e-03 ... -8.6052471e-04\n",
            "    2.2909241e-03 -3.6700661e-03]\n",
            "  ...\n",
            "  [ 1.0513142e-02 -7.7109965e-03 -3.1225802e-03 ...  3.9000702e-03\n",
            "   -1.9359861e-03 -8.4364810e-04]\n",
            "  [ 1.7211173e-02 -5.2137822e-03  5.7727811e-03 ...  1.1217708e-02\n",
            "   -4.2346474e-03 -1.0628265e-03]\n",
            "  [ 1.4104850e-02 -8.9182500e-03  3.3491217e-03 ...  1.1871304e-02\n",
            "   -2.7530701e-03 -2.5162206e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "id": "9ib3RBOn4Anc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78b5237-781a-44ed-e367-7c85e60dd283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 6.5990896e-03 -8.0491661e-04  2.9588069e-03 ...  1.7123790e-03\n",
            "   4.9040182e-03  8.1748265e-04]\n",
            " [ 2.9765735e-03  2.8438934e-03 -1.7314260e-03 ...  1.8159207e-03\n",
            "   5.7474291e-04  1.5131948e-03]\n",
            " [ 8.3435391e-04 -8.3201716e-04  2.9813726e-03 ... -1.1281332e-03\n",
            "   1.9498751e-03 -1.3327150e-04]\n",
            " ...\n",
            " [ 6.7952625e-03 -1.5051133e-03  5.0865868e-03 ...  5.4934071e-03\n",
            "  -8.4502632e-03 -3.2617158e-04]\n",
            " [ 6.1084516e-05  6.2189263e-04  7.4191783e-03 ...  5.2488940e-03\n",
            "  -4.2562382e-03 -3.0917411e-03]\n",
            " [-1.5618687e-03  2.3696423e-03  2.1944353e-03 ...  5.2982434e-03\n",
            "  -7.8450162e-03 -2.3223278e-03]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "id": "QSXI-cPV4Sri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44f85d5-d094-47a2-8a6a-d4d9c91d6443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 0.00659909 -0.00080492  0.00295881 -0.00318686  0.00218056  0.00967585\n",
            "  0.00321764  0.00388249 -0.00281956  0.00482339 -0.00733554  0.00316856\n",
            "  0.00023008 -0.00033126  0.00122418 -0.00654856  0.00133901  0.00220707\n",
            " -0.0032013  -0.00581087  0.00473592  0.00351033  0.00228146  0.00397234\n",
            " -0.00267837  0.00205467  0.00542691  0.00246941  0.00104776  0.00373582\n",
            "  0.00023682  0.00311069 -0.001994   -0.00501615 -0.00324747 -0.00028087\n",
            " -0.00308091 -0.00493471 -0.00173699 -0.00037338  0.00295488 -0.00144024\n",
            " -0.0039221   0.00599922 -0.00083556 -0.00654565  0.00252875  0.00233963\n",
            " -0.00315632 -0.0013314  -0.00317388 -0.00404066  0.00187268  0.00082768\n",
            " -0.00262197 -0.00258375 -0.00451852 -0.00050195 -0.00043584  0.00650063\n",
            "  0.00297665  0.01059148  0.00171238  0.00490402  0.00081748], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AXTi1i3r4lfr",
        "outputId": "4afc530e-0230-43a8-f375-b0948774c2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IH,fhoVHPW.NbT-jCb$Dj uCOBXpQ;.S$OU!zmvxWKLe;FPhRcByvqqcFtpwVg!o?HB$D$Iyg;KMtqYfj!vWmaSH&GqmPCZppAVL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "-AMeQhTk44Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "kNt7uGo-PX8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "wrQaNu-HPY25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaO-l3xEPgbK",
        "outputId": "824df85b-d3b8-44ba-8d96-b6064b69aed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 14s 63ms/step - loss: 2.7200\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 12s 65ms/step - loss: 2.0339\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 12s 64ms/step - loss: 1.7779\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 14s 65ms/step - loss: 1.6291\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 12s 65ms/step - loss: 1.5351\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 12s 65ms/step - loss: 1.4702\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.4248\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.3872\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.3560\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.3300\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.3050\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2816\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2600\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2392\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2191\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.1965\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.1752\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1532\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1293\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1059\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.0808\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0552\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0284\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0009\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9745\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9451\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9179\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.8892\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.8616\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.8356\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.8083\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7834\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.7600\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7346\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7140\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6935\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.6730\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.6547\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6362\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6201\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6036\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5913\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5774\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.5665\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5537\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.5420\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5324\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5238\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.5151\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.5059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "VrkXhLp2PkZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "YORAUuCSPwMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(\"./training_checkpoints/ckpt_\" + str(checkpoint_num))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "OWuGoJL-P1At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "qabMxAeEP4S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBZBYK1HP9we",
        "outputId": "bac1e7df-37d7-4daf-e68f-d01cb5a57ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: romeo\n",
            "romeoth, yet, must I lose the ceptreching\n",
            "rat, corruption'd herdors have fall on her heart\n",
            "A thought belong up's off the hopely and motion at kings\n",
            "Pirg into this bexate,\n",
            "Deteniers thy under hebe\n",
            "Our fetch hands.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Well and sit and in the Taway. Nor of an afterut?\n",
            "On we begin there.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Play's, sir, let me never title: no minion, know\n",
            "as he do lapeful throngs the To come:\n",
            "This arm let as fair wast this till leich ret her son may at marred thy device,\n",
            "That question after spools with unp the poor Sis\n",
            "Richamponion, sir, me how you must fellow and!\n",
            "Now, it shall be on his supposses?\n",
            "We are thy nobicious.\n",
            "\n",
            "LICINES:\n",
            "My lord and Warwick?\n",
            "\n",
            "GREMIO:\n",
            "Thou shalt not speak the thing voucheaties of serveal,\n",
            "If little hand.\n",
            "\n",
            "Servant:\n",
            "My vialent, naildo's gracious lady:\n",
            "IN Dose and \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXOP0spLc8Ck",
        "outputId": "e0295023-b2a7-4619-9143-c5908526958e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  training_checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd training_checkpoints\n"
      ],
      "metadata": {
        "id": "QTAczc5Dey1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4v0shrKe2XG",
        "outputId": "e17d372d-b4d1-4d2e-949a-8db55e7f94d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtraining_checkpoints\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMNnqeWUe3SM",
        "outputId": "13b8f3fa-ac25-462f-f492-4910ecddb035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rYSZ9BYce3zx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}